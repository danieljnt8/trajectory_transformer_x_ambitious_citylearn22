run_id :
  name : traj_config_v1_seq20_rf_CombinedReward_norm_wrapper #difference from v1 is seq len 

wandb:
  project: null  # write yours
  name: "city_learn"
  group: "gpt_cache"
  entity: null  # write yours
  mode: "disabled"  # set to online, if needed

dataset:
  env_name: "city_learn"
  seq_len: 20
  cache_path: "data"
  num_bins: ${model.vocab_size}
  discount: 0.99
  strategy: "uniform"
  batch_size: 64

model:
  vocab_size: 100
  transition_dim: 51
  observation_dim: 44
  action_dim: 5
  seq_len: 1020 ## transition dim x seq len in dataset
  embedding_dim: 128
  num_layers: 4
  num_heads: 4
  use_sep_heads: true

trainer:
  data_type : "model_RBCAgent1_timesteps_100000_rf_CombinedReward_seed_28_norm_wrapper"
  offline_data_path: "data_interactions/RBCAgent1/model_RBCAgent1_timesteps_8760_rf_CombinedReward_phase_2_8760.pkl"
  num_epochs : 200
  num_epochs_ref: 200
  action_weight: 5
  value_weight: 1
  reward_weight: 1
  lr: 6e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
  clip_grad: 1.0
  eval_seed: 42
  eval_every: 10
  eval_episodes: 5
  eval_discount: ${dataset.discount}
  eval_temperature: 1
  eval_plan_every: 1
  eval_beam_width: 32
  eval_beam_steps: 5
  eval_beam_context: 5
  eval_sample_expand: 2
  eval_k_obs: 1
  eval_k_reward: 1
  eval_k_act: null
  checkpoints_path: "checkpoints/${dataset.env_name}/${run_id.name}/${dataset.strategy}/${trainer.data_type}/run_7"